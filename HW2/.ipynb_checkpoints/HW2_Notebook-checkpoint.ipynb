{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfnWLBlXtyn6"
   },
   "source": [
    "# Deep Convolutional Generative Adversarial Networks with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9bRlEp-MWtX"
   },
   "source": [
    "This is the report and code (Jupyter notebook) for the second homework of the course \"Selected Topics in Visual Recognition in Deep Learning\"\n",
    "\n",
    "Student: Fu-sung Kim-Benjamin Tang\n",
    "Student-ID: 0845058\n",
    "\n",
    "# Github Link for the repository: \n",
    "https://github.com/kimbold/VRDL_2019\n",
    "\n",
    "\n",
    "# Brief introduction:\n",
    "For this homework we had to download the celebrity dataset and then train a Generative Adversarial Network on it to generate 500 different 3x3 images with faces (so 9 faces per image). \n",
    "\n",
    "For this a generator, which will generate the faces and a discrimantor, which will discriminate between real images and generated ones, are required.\n",
    "The idea basically two networks are trained to work together and the generator becomes increasingly better at generating realisticly looking faces whereas the disriminator becomes better as differentiating between generated ones and real ones. By doing so, both systems will improve and end up creating quite realistic faces (hopefully). \n",
    "\n",
    "This notebook was executed on Google colab and I established a connection with my drive to store the images there. If it is not executed in google colab, the lines for that have to be removed/commented.\n",
    "\n",
    "# Methodology:\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "\n",
    "*   Load images with data helper\n",
    "*   Define method to get batches from image data\n",
    "*   Define inputs for network\n",
    "\n",
    "\n",
    "**Model architecture:**\n",
    "\n",
    "* Generator network with 4 convolutional tensorflow layers\n",
    "* Discriminator network with 4 convolutional tensorflow layers\n",
    "* Data input configured for image size (56x56)\n",
    "\n",
    "**Hyperparamters:**\n",
    "\n",
    "*Images:*\n",
    "*   Image height/width = 56x56\n",
    "*   Colormode = RGB\n",
    "\n",
    "*Networks:*\n",
    "* batch_size = 16\n",
    "* z_dim = 100\n",
    "* learning_rate = 0.0002\n",
    "* beta1 = 0.5\n",
    "* epochs = 20\n",
    "* alpha=0.2\n",
    "* label_smoothing = 0.9\n",
    "* leaky ReLu\n",
    "* 4 convolutional layers\n",
    "\n",
    "# Findings/Summary:\n",
    "\n",
    "Its my first time experimenting with a Generative Adversial Network and also to generate faces, so it was surprising how well it worked, since most generated faces look not too bad. But it is notable that in almost every image with the 9 faces, at least one looks really off/is poorly generated. There is definitely a lot that could be improved but I tried to increase the number of epochs just to find out, that at around 35 the performance suddenly drops heavily. \n",
    "It looked like the GAN suddenly forgot everything and started generating really bad faces. So I assume that there is a point until training it with more epochs improves performances until it degrades it. So early stopping would be a measurement to prevent this in the future. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZJjXTe6tyoF"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRDq4UrNtyoH"
   },
   "source": [
    "\n",
    "\n",
    "1.   Get helper method for dataset download from my github repository\n",
    "1.   Import required modules together with helper\n",
    "2.   Download and extract celeb dataset into data folder\n",
    "3. Define methods to get images (as matrices) and data as batches \n",
    "4. Define network input, discriminator, generator, loss function and optimization\n",
    "5. Train\n",
    "6. After model is finished with training, use it together with the output method to create 500 images with 3x3 generated images on them as required by the homework\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Used Libraries:\n",
    "* matplotlib\n",
    "* PIL / Pillow\n",
    "* numpy\n",
    "* requests\n",
    "* tqdm\n",
    "* TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "LPemKr6PuEQh",
    "outputId": "acc903fb-666a-4f83-b6ba-ff460b7a4299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VRDL_2019'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects:   5% (1/20)\u001b[K\r",
      "remote: Counting objects:  10% (2/20)\u001b[K\r",
      "remote: Counting objects:  15% (3/20)\u001b[K\r",
      "remote: Counting objects:  20% (4/20)\u001b[K\r",
      "remote: Counting objects:  25% (5/20)\u001b[K\r",
      "remote: Counting objects:  30% (6/20)\u001b[K\r",
      "remote: Counting objects:  35% (7/20)\u001b[K\r",
      "remote: Counting objects:  40% (8/20)\u001b[K\r",
      "remote: Counting objects:  45% (9/20)\u001b[K\r",
      "remote: Counting objects:  50% (10/20)\u001b[K\r",
      "remote: Counting objects:  55% (11/20)\u001b[K\r",
      "remote: Counting objects:  60% (12/20)\u001b[K\r",
      "remote: Counting objects:  65% (13/20)\u001b[K\r",
      "remote: Counting objects:  70% (14/20)\u001b[K\r",
      "remote: Counting objects:  75% (15/20)\u001b[K\r",
      "remote: Counting objects:  80% (16/20)\u001b[K\r",
      "remote: Counting objects:  85% (17/20)\u001b[K\r",
      "remote: Counting objects:  90% (18/20)\u001b[K\r",
      "remote: Counting objects:  95% (19/20)\u001b[K\r",
      "remote: Counting objects: 100% (20/20)\u001b[K\r",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects:   5% (1/18)\u001b[K\r",
      "remote: Compressing objects:  11% (2/18)\u001b[K\r",
      "remote: Compressing objects:  16% (3/18)\u001b[K\r",
      "remote: Compressing objects:  22% (4/18)\u001b[K\r",
      "remote: Compressing objects:  27% (5/18)\u001b[K\r",
      "remote: Compressing objects:  33% (6/18)\u001b[K\r",
      "remote: Compressing objects:  38% (7/18)\u001b[K\r",
      "remote: Compressing objects:  44% (8/18)\u001b[K\r",
      "remote: Compressing objects:  50% (9/18)\u001b[K\r",
      "remote: Compressing objects:  55% (10/18)\u001b[K\r",
      "remote: Compressing objects:  61% (11/18)\u001b[K\r",
      "remote: Compressing objects:  66% (12/18)\u001b[K\r",
      "remote: Compressing objects:  72% (13/18)\u001b[K\r",
      "remote: Compressing objects:  77% (14/18)\u001b[K\r",
      "remote: Compressing objects:  83% (15/18)\u001b[K\r",
      "remote: Compressing objects:  88% (16/18)\u001b[K\r",
      "remote: Compressing objects:  94% (17/18)\u001b[K\r",
      "remote: Compressing objects: 100% (18/18)\u001b[K\r",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "Receiving objects:   0% (1/3885)   \r",
      "Receiving objects:   1% (39/3885)   \r",
      "Receiving objects:   2% (78/3885)   \r",
      "Receiving objects:   3% (117/3885)   \r",
      "Receiving objects:   4% (156/3885)   \r",
      "Receiving objects:   5% (195/3885)   \r",
      "Receiving objects:   6% (234/3885)   \r",
      "Receiving objects:   7% (272/3885)   \r",
      "Receiving objects:   8% (311/3885)   \r",
      "Receiving objects:   9% (350/3885)   \r",
      "Receiving objects:  10% (389/3885)   \r",
      "Receiving objects:  11% (428/3885)   \r",
      "Receiving objects:  12% (467/3885)   \r",
      "Receiving objects:  13% (506/3885)   \r",
      "Receiving objects:  14% (544/3885)   \r",
      "Receiving objects:  15% (583/3885)   \r",
      "Receiving objects:  16% (622/3885)   \r",
      "Receiving objects:  17% (661/3885)   \r",
      "Receiving objects:  18% (700/3885)   \r",
      "Receiving objects:  19% (739/3885)   \r",
      "Receiving objects:  20% (777/3885)   \r",
      "Receiving objects:  21% (816/3885)   \r",
      "Receiving objects:  22% (855/3885)   \r",
      "Receiving objects:  23% (894/3885)   \r",
      "Receiving objects:  24% (933/3885)   \r",
      "Receiving objects:  25% (972/3885)   \r",
      "Receiving objects:  26% (1011/3885)   \r",
      "Receiving objects:  27% (1049/3885)   \r",
      "Receiving objects:  28% (1088/3885)   \r",
      "Receiving objects:  29% (1127/3885)   \r",
      "Receiving objects:  30% (1166/3885)   \r",
      "Receiving objects:  31% (1205/3885)   \r",
      "Receiving objects:  32% (1244/3885)   \r",
      "Receiving objects:  33% (1283/3885)   \r",
      "Receiving objects:  34% (1321/3885)   \r",
      "Receiving objects:  35% (1360/3885)   \r",
      "Receiving objects:  36% (1399/3885)   \r",
      "Receiving objects:  37% (1438/3885)   \r",
      "Receiving objects:  38% (1477/3885)   \r",
      "Receiving objects:  39% (1516/3885)   \r",
      "Receiving objects:  40% (1554/3885)   \r",
      "Receiving objects:  41% (1593/3885)   \r",
      "Receiving objects:  42% (1632/3885)   \r",
      "Receiving objects:  43% (1671/3885)   \r",
      "Receiving objects:  44% (1710/3885)   \r",
      "Receiving objects:  45% (1749/3885)   \r",
      "Receiving objects:  46% (1788/3885)   \r",
      "Receiving objects:  47% (1826/3885)   \r",
      "Receiving objects:  48% (1865/3885)   \r",
      "Receiving objects:  49% (1904/3885)   \r",
      "Receiving objects:  50% (1943/3885)   \r",
      "Receiving objects:  51% (1982/3885)   \r",
      "Receiving objects:  52% (2021/3885)   \r",
      "Receiving objects:  53% (2060/3885)   \r",
      "Receiving objects:  54% (2098/3885)   \r",
      "Receiving objects:  55% (2137/3885)   \r",
      "Receiving objects:  56% (2176/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  57% (2215/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  58% (2254/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  59% (2293/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  60% (2331/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  61% (2370/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  62% (2409/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  63% (2448/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  64% (2487/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  65% (2526/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  66% (2565/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  67% (2603/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  68% (2642/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  69% (2681/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  70% (2720/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  71% (2759/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  72% (2798/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  73% (2837/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  74% (2875/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  75% (2914/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  76% (2953/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  77% (2992/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  78% (3031/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  79% (3070/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  80% (3108/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  81% (3147/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  82% (3186/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  83% (3225/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  84% (3264/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  85% (3303/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  86% (3342/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  87% (3380/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  88% (3419/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  89% (3458/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  90% (3497/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  91% (3536/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  92% (3575/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  93% (3614/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  94% (3652/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  95% (3691/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  96% (3730/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  97% (3769/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  98% (3808/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects:  99% (3847/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "remote: Total 3885 (delta 1), reused 16 (delta 0), pack-reused 3865\u001b[K\n",
      "Receiving objects: 100% (3885/3885), 29.17 MiB | 58.33 MiB/s   \r",
      "Receiving objects: 100% (3885/3885), 52.45 MiB | 56.65 MiB/s, done.\n",
      "Resolving deltas:   0% (0/1)   \r",
      "Resolving deltas: 100% (1/1)   \r",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kimbold/VRDL_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "OH1YtgZ3tyoM",
    "outputId": "73a9678d-d8d3-49ab-8146-2b56f15dcd3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading celeba: 1.44GB [00:18, 77.0MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting celeba...\n",
      "(9, 56, 56, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import VRDL_2019.HW2.helper as helper\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "data_dir = './data'\n",
    "helper.download_extract(data_dir)\n",
    "\n",
    "show_n_images = 9\n",
    "image_size = 56\n",
    "plt.figure(figsize=(10, 10))\n",
    "images = helper.get_batch(glob(os.path.join(data_dir, 'img_align_celeba/*.jpg'))[:show_n_images], image_size, image_size, 'RGB')\n",
    "#plt.imshow(helper.images_square_grid(images))\n",
    "#plt.show()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBOZKLZutyoY"
   },
   "source": [
    "## The CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "IbYHWtNItyob",
    "outputId": "0fe10c57-4de8-4b15-e5f6-55e6316d4a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (202599, 56, 56, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fac4a6146d8>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJYFNdoOhJLDJbjCUBP8f0ZJCMw/IRIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Image configuration with height, width and storage location\n",
    "IMAGE_HEIGHT = 56\n",
    "IMAGE_WIDTH = 56\n",
    "data_files = glob(os.path.join(data_dir, 'img_align_celeba/*.jpg'))\n",
    "shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, 3\n",
    "\n",
    "print(\"shape:\", shape)\n",
    "\n",
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if image.size != (width, height):\n",
    "        # Remove most pixels that aren't part of a face\n",
    "        face_width = face_height = 108\n",
    "        j = (image.size[0] - face_width) // 2\n",
    "        i = (image.size[1] - face_height) // 2\n",
    "        image = image.crop([j, i, j + face_width, i + face_height])\n",
    "        image = image.resize([width, height], Image.BILINEAR)\n",
    "\n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "def get_batch(image_files, width, height, mode='RGB'):\n",
    "    \"\"\"\n",
    "    Get a single image\n",
    "    \"\"\"\n",
    "    data_batch = np.array([get_image(sample_file, width, height, mode) for sample_file in image_files]).astype(np.float32)\n",
    "\n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "\n",
    "    return data_batch\n",
    "\n",
    "def get_batches(batch_size):\n",
    "  #print(\"Hello\")\n",
    "  IMAGE_MAX_VALUE = 255\n",
    "  current_index = 0\n",
    "\n",
    "  while current_index + batch_size <= shape[0]:\n",
    "        #print(shape[0])\n",
    "        data_batch = get_batch(data_files[current_index:current_index + batch_size],*shape[1:3])\n",
    "\n",
    "        current_index += batch_size\n",
    "\n",
    "        yield data_batch / IMAGE_MAX_VALUE - 0.5\n",
    "        \n",
    "\n",
    "#test_images = get_batch(glob(os.path.join(data_dir, 'celebA/*.jpg'))[:10], 56, 56)\n",
    "pyplot.imshow(get_image(\"data/img_align_celeba/006917.jpg\", IMAGE_WIDTH, IMAGE_HEIGHT, 'RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "98j2L47ZwxNt",
    "outputId": "a716364a-1aea-440a-fc69-dac1269807f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 56, 56, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now get all images with the define properties\n",
    "show_n_images = 9\n",
    "image_size = 56\n",
    "plt.figure(figsize=(10, 10))\n",
    "images = helper.get_batch(glob(os.path.join(data_dir, 'img_align_celeba/*.jpg'))[:show_n_images], image_size, image_size, 'RGB')\n",
    "#plt.imshow(helper.images_square_grid(images))\n",
    "#plt.show()\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vp6urCqetyoj"
   },
   "source": [
    "## Defining network input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kd681xYNtyol"
   },
   "source": [
    "Before defining the two networks, the inputs must be defined. TensorFlow Placeholders for the real and fake inputs and for the learning rate are going to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "oCfVgn-wtyon",
    "outputId": "cd0a5359-3f55-4821-e964-38704b7c4c9b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_real = tf.placeholder(tf.float32, shape=(None, image_width, image_height, image_channels), name='input_real') \n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return inputs_real, inputs_z, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91GxRwQGtyot"
   },
   "source": [
    "## The discriminator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3jGCMpytyov"
   },
   "source": [
    "The discriminator distinguishes between real and generated images. In essence it is a convolutional neural network for image classification. The discriminator network consists of convolutional layers and for every layer of the network, a convolution, a batch normalization to make the network faster and more accurate and finally a Leaky ReLu are going to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhrAhdGttyox"
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False):\n",
    "    \"\"\"\n",
    "    Create the discriminator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # using 4 layer network as in DCGAN Paper\n",
    "        \n",
    "        # Conv 1\n",
    "        conv1 = tf.layers.conv2d(images, 64, 5, 2, 'SAME')\n",
    "        lrelu1 = tf.maximum(alpha * conv1, conv1)\n",
    "        \n",
    "        # Conv 2\n",
    "        conv2 = tf.layers.conv2d(lrelu1, 128, 5, 2, 'SAME')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training=True)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Conv 3\n",
    "        conv3 = tf.layers.conv2d(lrelu2, 256, 5, 1, 'SAME')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training=True)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "\n",
    "        # Conv 4\n",
    "        conv4 = tf.layers.conv2d(lrelu3, 512, 5, 1, 'SAME')\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4, training=True)\n",
    "        lrelu4 = tf.maximum(alpha * batch_norm4, batch_norm4)\n",
    "       \n",
    "        # Flatten\n",
    "        flat = tf.reshape(lrelu4, (-1, 4*4*256))\n",
    "        \n",
    "        # Logits\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        \n",
    "        # Output\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e36ZTeSntyo3"
   },
   "source": [
    "## The generator network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBxVS57Dtyo5"
   },
   "source": [
    "The generator goes the other way: It is the artist who is trying to fool the discriminator. This network consists of four deconvolutional layers. In here, we are doing the same as in the discriminator, just in the other direction. First, we take our input, called Z, and feed it into our first deconvolutional layer. Each deconvolutional layer performs a deconvolution and then performs batch normalization and a leaky ReLu as well. Then, we return the tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pqhoD4Jtyo7"
   },
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse=False if is_train==True else True):\n",
    "        # First fully connected layer\n",
    "        x_1 = tf.layers.dense(z, 2*2*512)\n",
    "        \n",
    "        # Reshape it to start the convolutional stack\n",
    "        deconv_2 = tf.reshape(x_1, (-1, 2, 2, 512))\n",
    "        batch_norm2 = tf.layers.batch_normalization(deconv_2, training=is_train)\n",
    "        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)\n",
    "        \n",
    "        # Deconv 1\n",
    "        deconv3 = tf.layers.conv2d_transpose(lrelu2, 256, 5, 2, padding='VALID')\n",
    "        batch_norm3 = tf.layers.batch_normalization(deconv3, training=is_train)\n",
    "        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)\n",
    "        \n",
    "        \n",
    "        # Deconv 2\n",
    "        deconv4 = tf.layers.conv2d_transpose(lrelu3, 128, 5, 2, padding='SAME')\n",
    "        batch_norm4 = tf.layers.batch_normalization(deconv4, training=is_train)\n",
    "        lrelu4 = tf.maximum(alpha * batch_norm4, batch_norm4)\n",
    "\n",
    "        # Deconv 3\n",
    "        deconv5 = tf.layers.conv2d_transpose(lrelu4, 64, 5, 2, padding='SAME')\n",
    "        batch_norm5 = tf.layers.batch_normalization(deconv5, training=is_train)\n",
    "        lrelu5 = tf.maximum(alpha * batch_norm5, batch_norm5)\n",
    "\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.conv2d_transpose(lrelu5, out_channel_dim, 5, 2, padding='SAME')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTzsNZVqtypB"
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNKLU_RctypG"
   },
   "source": [
    "Rather than just having a single loss function, we need to define three: The loss of the generator, the loss of the discriminator when using real images and the loss of the discriminator when using fake images. The sum of the fake image and real image loss is the overall discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR6cbYKwtypJ"
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    \"\"\"\n",
    "    \n",
    "    label_smoothing = 0.9\n",
    "    \n",
    "    g_model = generator(input_z, out_channel_dim)\n",
    "    d_model_real, d_logits_real = discriminator(input_real)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                labels=tf.ones_like(d_model_real) * label_smoothing))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "                                                  \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.ones_like(d_model_fake) * label_smoothing))\n",
    "    \n",
    "    \n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLko4KjhtypR"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmqexxYStypT"
   },
   "source": [
    "Rather than just having a single loss function, we need to define three: The loss of the generator, the loss of the discriminator when using real images and the loss of the discriminator when using fake images. The sum of the fake image and real image loss is the overall discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMg9PV5ItypV"
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    \"\"\"\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJZ8URrgtypb"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sC8ZAwvQtype"
   },
   "source": [
    "\n",
    "In the last step of our preparation, we are writing a small helper function to display the generated images in the notebook for us, using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcwHsoBPtyph"
   },
   "outputs": [],
   "source": [
    "def output_fig(images_array, file_name=\"./results\"):\n",
    "  pyplot.figure(figsize=(6,6), dpi=100)\n",
    "  pyplot.imshow(helper.images_square_grid(images_array))\n",
    "  pyplot.axis(\"off\")\n",
    "  pyplot.savefig(file_name+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "def show_generator_output(sess, n_images, input_z, out_channel_dim, save_option, id):\n",
    "    \"\"\"\n",
    "    Show example output for the generator\n",
    "    \"\"\"\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "  \n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "\n",
    "    if(save_option==True):\n",
    "      file_name=\"{:03d}\".format(id)\n",
    "      output_fig(samples, file_name=file_name)\n",
    "      uploaded = drive.CreateFile({'title': file_name+\".png\"})\n",
    "      uploaded.SetContentFile(file_name+\".png\")\n",
    "      uploaded.Upload()\n",
    "      print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
    "    else:\n",
    "      pyplot.imshow(helper.images_square_grid(samples))\n",
    "      pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u05tWhRktypq"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NveoOn8wtypv"
   },
   "source": [
    "Now by using the inputs, losses and optimizers as defined before, a TensorFlow session will be called and executed batch by batch. Every 400 steps the current progress will be printed out by showing the generated image and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuyRRjx4typz"
   },
   "outputs": [],
   "source": [
    "def train(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape):\n",
    "    \"\"\"\n",
    "    Train the GAN\n",
    "    \"\"\"\n",
    "    input_real, input_z, _ = model_inputs(data_shape[1], data_shape[2], data_shape[3], z_dim)\n",
    "    d_loss, g_loss = model_loss(input_real, input_z, data_shape[3])\n",
    "    d_opt, g_opt = model_opt(d_loss, g_loss, learning_rate, beta1)\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            print(\"Epoch:\", epoch_i, \"/\", epoch_count)\n",
    "            for batch_images in get_batches(batch_size):\n",
    "                \n",
    "                # values range from -0.5 to 0.5, therefore scale to range -1, 1\n",
    "                batch_images = batch_images * 2\n",
    "                steps += 1\n",
    "            \n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "                \n",
    "                _ = sess.run(d_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "                _ = sess.run(g_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "                \n",
    "                if steps % 400 == 0:\n",
    "                    train_loss_d = d_loss.eval({input_z: batch_z, input_real: batch_images})\n",
    "                    train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "\n",
    "                    print(\"Epoch {}/{}...\".format(epoch_i+1, epochs),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                   \n",
    "                    _ = show_generator_output(sess, 9, input_z, data_shape[3], False, 0)\n",
    "\n",
    "        for x in range(500):\n",
    "           _ = show_generator_output(sess, 9, input_z, data_shape[3], True, x)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Uh4WKU9qtyp8",
    "outputId": "87d22ed8-be6f-4f08-b045-6e3e8ed39b06"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "z_dim = 100\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "epochs = 20\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once in a notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once in a notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "#directory=\"GeneratedImages\"\n",
    "#if not os.path.exists(directory):\n",
    "#    os.makedirs(directory)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, get_batches, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "584ViOxAJ9U1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGANs with Tensorflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
